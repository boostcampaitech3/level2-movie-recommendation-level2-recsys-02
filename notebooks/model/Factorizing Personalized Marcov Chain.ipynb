{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy import sparse\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df) :\n",
    "    print(\"preprocessing..\")\n",
    "    df = df.sort_values(['user', 'time'], ascending = [True, True])\n",
    "\n",
    "    users = df['user'].unique()\n",
    "    user_to_id = dict(zip(users, range(len(users))))\n",
    "    id_to_user = {v: k for k, v in user_to_id.items()}\n",
    "    \n",
    "    movies = df['item'].unique()\n",
    "    movie_to_id = dict(zip(movies, range(len(movies))))\n",
    "    id_to_movie = {v: k for k, v in movie_to_id.items()}\n",
    "    \n",
    "    df['user'] = df['user'].apply(lambda x : user_to_id[x])\n",
    "    df['item'] = df['item'].apply(lambda x : movie_to_id[x])\n",
    "\n",
    "    return df, user_to_id, id_to_user, movie_to_id, id_to_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1230782529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1230782534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1230782539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1230782542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1230782563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154466</th>\n",
       "      <td>31359</td>\n",
       "      <td>423</td>\n",
       "      <td>1260209449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154467</th>\n",
       "      <td>31359</td>\n",
       "      <td>1491</td>\n",
       "      <td>1260209482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154468</th>\n",
       "      <td>31359</td>\n",
       "      <td>331</td>\n",
       "      <td>1260209720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154469</th>\n",
       "      <td>31359</td>\n",
       "      <td>733</td>\n",
       "      <td>1260209726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154470</th>\n",
       "      <td>31359</td>\n",
       "      <td>2256</td>\n",
       "      <td>1260209807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5154471 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  item        time\n",
       "0            0     0  1230782529\n",
       "1            0     1  1230782534\n",
       "2            0     2  1230782539\n",
       "3            0     3  1230782542\n",
       "4            0     4  1230782563\n",
       "...        ...   ...         ...\n",
       "5154466  31359   423  1260209449\n",
       "5154467  31359  1491  1260209482\n",
       "5154468  31359   331  1260209720\n",
       "5154469  31359   733  1260209726\n",
       "5154470  31359  2256  1260209807\n",
       "\n",
       "[5154471 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/opt/ml/movie-recommendation/data/train/'\n",
    "\n",
    "df = pd.read_csv(data_dir+'train_ratings.csv')\n",
    "df, user_to_id, id_to_user, movie_to_id, id_to_movie = preprocess(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPMC(nn.Module):\n",
    "    def __init__(self, user_num, item_num, factor_num):\n",
    "        super(FPMC, self).__init__()\t\n",
    "        self.embed_UI = nn.Embedding(user_num, factor_num)\n",
    "        self.embed_IU = nn.Embedding(item_num, factor_num)\n",
    "        self.embed_LI = nn.Embedding(item_num+1, factor_num, padding_idx=0)\n",
    "        self.embed_IL = nn.Embedding(item_num, factor_num)\n",
    "        \n",
    "        nn.init.normal_(self.embed_UI.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_IU.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_LI.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_IL.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user, item, item_seq, seq_len):\n",
    "        VUI = self.embed_UI(user) # (batch_size, factor_num)\n",
    "        VIU = self.embed_IU(item) # (batch_size, factor_num)\n",
    "        VLI = self.embed_LI(item_seq) # (batch_size, sequence_len, factor_num)\n",
    "        VIL = self.embed_IL(item) # (batch_size, factor_num)\n",
    "        # (batch_size, seq_len,)\n",
    "\n",
    "        VUI_m_VIU = torch.sum(VUI*VIU, axis=1)\n",
    "        VLI_m_VIL = torch.sum(torch.bmm(VLI, (VIL.unsqueeze(2))), axis=1) / seq_len.unsqueeze(1)\n",
    "\n",
    "        return VUI_m_VIU + VLI_m_VIL.squeeze()\n",
    "    \n",
    "    def predict(self, user, items, item_seq, seq_len):\n",
    "        VUI = self.embed_UI(user) # (batch_size, factor_num)\n",
    "        VIU = self.embed_IU(items) # (batch_size, item_num, factor_num)\n",
    "        VLI = self.embed_LI(item_seq) # (batch_size, sequence_len, factor_num)\n",
    "        VIL = self.embed_IL(items) # (batch_size, item_num, factor_num)\n",
    "        # (batch_size, seq_len,)\n",
    "\n",
    "        VUI_m_VIU = torch.bmm(VIU, VUI.unsqueeze(2))\n",
    "        VLI_m_VIL = torch.sum(torch.bmm(VIL, VLI.transpose(1,2)), axis=2) / seq_len.unsqueeze(1)\n",
    "\n",
    "        return VUI_m_VIU.squeeze() + VLI_m_VIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDataset(Dataset):\n",
    "    def __init__(self, data, num_negative=10, is_training=False) :\n",
    "        self.data = data[['user', 'item']]\n",
    "        self.n_user = self.data['user'].nunique() \n",
    "        self.n_item = self.data['item'].nunique()\n",
    "        self.num_negative = num_negative\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.user2seq = dict()\n",
    "        user_item_sequence = list(self.data.groupby(by='user')['item'])\n",
    "        for user, item_seq in user_item_sequence :\n",
    "            self.user2seq[user] = list(item_seq)\n",
    "        \n",
    "        if not self.is_training :\n",
    "            self.users = list(self.user2seq.keys())\n",
    "            self.item_seqs = list(self.user2seq.values())\n",
    "\n",
    "    def negative_sampling(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "        negative_samples = []\n",
    "        \n",
    "        for u, i in self.data.values:\n",
    "            for _ in range(self.num_negative):\n",
    "                j = np.random.randint(self.n_item)\n",
    "                while j in self.user2seq[u]:\n",
    "                    j = np.random.randint(self.n_item)\n",
    "                negative_samples.append([u, i, j])\n",
    "        self.features = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_negative * len(self.data) if self.is_training else self.n_user\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"user\":torch.tensor(self.features[idx][0]), \n",
    "                \"pos_item\": torch.tensor(self.features[idx][1]),\n",
    "                \"neg_item\": torch.tensor(self.features[idx][2]),\n",
    "                \"item_seq\": torch.tensor(\n",
    "                    list(set(self.user2seq[self.features[idx][0]]) - \\\n",
    "                    set([self.features[idx][1]]))\n",
    "                ),\n",
    "                \"seq_len\": torch.tensor(len(self.user2seq[self.features[idx][0]])-1),}\\\n",
    "                if self.is_training else \\\n",
    "                {\"user\":torch.tensor(self.users[idx]),\n",
    "                \"pos_item\": torch.arange(0,self.n_item),\n",
    "                \"neg_item\": torch.tensor([0]),\n",
    "                \"item_seq\": torch.tensor(self.item_seqs[idx]),\n",
    "                \"seq_len\": torch.tensor(len(self.item_seqs[idx])),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(samples):\n",
    "    users = [sample['user'] for sample in samples]\n",
    "    pos_items = [sample['pos_item'] for sample in samples]\n",
    "    neg_items = [sample['neg_item'] for sample in samples]\n",
    "    seq_lens = [sample['seq_len']+1 for sample in samples]\n",
    "    item_seqs = [sample['item_seq'] for sample in samples]\n",
    "\n",
    "    padded_item_seqs = torch.nn.utils.rnn.pad_sequence(item_seqs, batch_first=True)\n",
    "    return {'user': torch.stack(users).contiguous(),\n",
    "            'pos_item': torch.stack(pos_items).contiguous(),\n",
    "            'neg_item': torch.stack(neg_items).contiguous(),\n",
    "            'item_seq': padded_item_seqs.contiguous(),\n",
    "            'seq_len': torch.stack(seq_lens).contiguous()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequentialDataset(df, num_negative=10, is_training=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=True, \n",
    "    drop_last=True,\n",
    "    collate_fn=make_batch,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FPMC(train_dataset.n_user, train_dataset.n_item, 10).to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sampling ... Done!!\n",
      "Epoch[0/10](500/50336) || training loss 498.909 ||\n",
      "Epoch[0/10](1000/50336) || training loss 310.1986 ||\n",
      "Epoch[0/10](1500/50336) || training loss 296.0072 ||\n",
      "Epoch[0/10](2000/50336) || training loss 291.3797 ||\n",
      "Epoch[0/10](2500/50336) || training loss 283.443 ||\n",
      "Epoch[0/10](3000/50336) || training loss 277.427 ||\n",
      "Epoch[0/10](3500/50336) || training loss 267.8326 ||\n",
      "Epoch[0/10](4000/50336) || training loss 260.818 ||\n",
      "Epoch[0/10](4500/50336) || training loss 255.1404 ||\n",
      "Epoch[0/10](5000/50336) || training loss 247.598 ||\n",
      "Epoch[0/10](5500/50336) || training loss 242.8434 ||\n",
      "Epoch[0/10](6000/50336) || training loss 237.694 ||\n",
      "Epoch[0/10](6500/50336) || training loss 233.9127 ||\n",
      "Epoch[0/10](7000/50336) || training loss 229.0778 ||\n",
      "Epoch[0/10](7500/50336) || training loss 225.8099 ||\n",
      "Epoch[0/10](8000/50336) || training loss 223.1073 ||\n",
      "Epoch[0/10](8500/50336) || training loss 218.594 ||\n",
      "Epoch[0/10](9000/50336) || training loss 215.8828 ||\n",
      "Epoch[0/10](9500/50336) || training loss 213.6846 ||\n",
      "Epoch[0/10](10000/50336) || training loss 211.8823 ||\n",
      "Epoch[0/10](10500/50336) || training loss 208.035 ||\n",
      "Epoch[0/10](11000/50336) || training loss 206.3056 ||\n",
      "Epoch[0/10](11500/50336) || training loss 205.595 ||\n",
      "Epoch[0/10](12000/50336) || training loss 203.2932 ||\n",
      "Epoch[0/10](12500/50336) || training loss 201.5491 ||\n",
      "Epoch[0/10](13000/50336) || training loss 200.7641 ||\n",
      "Epoch[0/10](13500/50336) || training loss 198.8442 ||\n",
      "Epoch[0/10](14000/50336) || training loss 197.8803 ||\n",
      "Epoch[0/10](14500/50336) || training loss 197.3656 ||\n",
      "Epoch[0/10](15000/50336) || training loss 195.0949 ||\n",
      "Epoch[0/10](15500/50336) || training loss 194.4956 ||\n",
      "Epoch[0/10](16000/50336) || training loss 191.9469 ||\n",
      "Epoch[0/10](16500/50336) || training loss 192.4471 ||\n",
      "Epoch[0/10](17000/50336) || training loss 189.9382 ||\n",
      "Epoch[0/10](17500/50336) || training loss 190.4563 ||\n",
      "Epoch[0/10](18000/50336) || training loss 188.4583 ||\n",
      "Epoch[0/10](18500/50336) || training loss 187.8632 ||\n",
      "Epoch[0/10](19000/50336) || training loss 187.5558 ||\n",
      "Epoch[0/10](19500/50336) || training loss 186.6164 ||\n",
      "Epoch[0/10](20000/50336) || training loss 186.2345 ||\n",
      "Epoch[0/10](20500/50336) || training loss 184.4654 ||\n",
      "Epoch[0/10](21000/50336) || training loss 181.8471 ||\n",
      "Epoch[0/10](21500/50336) || training loss 182.429 ||\n",
      "Epoch[0/10](22000/50336) || training loss 181.4862 ||\n",
      "Epoch[0/10](22500/50336) || training loss 180.0786 ||\n",
      "Epoch[0/10](23000/50336) || training loss 180.0759 ||\n",
      "Epoch[0/10](23500/50336) || training loss 179.0977 ||\n",
      "Epoch[0/10](24000/50336) || training loss 177.6246 ||\n",
      "Epoch[0/10](24500/50336) || training loss 177.0829 ||\n",
      "Epoch[0/10](25000/50336) || training loss 176.1934 ||\n",
      "Epoch[0/10](25500/50336) || training loss 174.611 ||\n",
      "Epoch[0/10](26000/50336) || training loss 175.1259 ||\n",
      "Epoch[0/10](26500/50336) || training loss 173.389 ||\n",
      "Epoch[0/10](27000/50336) || training loss 173.8365 ||\n",
      "Epoch[0/10](27500/50336) || training loss 171.1836 ||\n",
      "Epoch[0/10](28000/50336) || training loss 172.3714 ||\n",
      "Epoch[0/10](28500/50336) || training loss 171.2463 ||\n",
      "Epoch[0/10](29000/50336) || training loss 170.5254 ||\n",
      "Epoch[0/10](29500/50336) || training loss 170.3636 ||\n",
      "Epoch[0/10](30000/50336) || training loss 168.6835 ||\n",
      "Epoch[0/10](30500/50336) || training loss 167.66 ||\n",
      "Epoch[0/10](31000/50336) || training loss 168.1039 ||\n",
      "Epoch[0/10](31500/50336) || training loss 167.4457 ||\n",
      "Epoch[0/10](32000/50336) || training loss 166.9152 ||\n",
      "Epoch[0/10](32500/50336) || training loss 165.708 ||\n",
      "Epoch[0/10](33000/50336) || training loss 165.1454 ||\n",
      "Epoch[0/10](33500/50336) || training loss 164.6051 ||\n",
      "Epoch[0/10](34000/50336) || training loss 165.1708 ||\n",
      "Epoch[0/10](34500/50336) || training loss 164.0202 ||\n",
      "Epoch[0/10](35000/50336) || training loss 163.9029 ||\n",
      "Epoch[0/10](35500/50336) || training loss 163.016 ||\n",
      "Epoch[0/10](36000/50336) || training loss 162.6274 ||\n",
      "Epoch[0/10](36500/50336) || training loss 162.5745 ||\n",
      "Epoch[0/10](37000/50336) || training loss 161.6719 ||\n",
      "Epoch[0/10](37500/50336) || training loss 161.0202 ||\n",
      "Epoch[0/10](38000/50336) || training loss 160.568 ||\n",
      "Epoch[0/10](38500/50336) || training loss 159.9308 ||\n",
      "Epoch[0/10](39000/50336) || training loss 160.0833 ||\n",
      "Epoch[0/10](39500/50336) || training loss 158.5331 ||\n",
      "Epoch[0/10](40000/50336) || training loss 158.9376 ||\n",
      "Epoch[0/10](40500/50336) || training loss 158.8914 ||\n",
      "Epoch[0/10](41000/50336) || training loss 158.3878 ||\n",
      "Epoch[0/10](41500/50336) || training loss 157.7388 ||\n",
      "Epoch[0/10](42000/50336) || training loss 157.7478 ||\n",
      "Epoch[0/10](42500/50336) || training loss 156.8614 ||\n",
      "Epoch[0/10](43000/50336) || training loss 155.7268 ||\n",
      "Epoch[0/10](43500/50336) || training loss 154.8643 ||\n",
      "Epoch[0/10](44000/50336) || training loss 155.2793 ||\n",
      "Epoch[0/10](44500/50336) || training loss 153.3609 ||\n",
      "Epoch[0/10](45000/50336) || training loss 154.7129 ||\n",
      "Epoch[0/10](45500/50336) || training loss 153.5765 ||\n",
      "Epoch[0/10](46000/50336) || training loss 153.4909 ||\n",
      "Epoch[0/10](46500/50336) || training loss 152.7089 ||\n",
      "Epoch[0/10](47000/50336) || training loss 152.929 ||\n",
      "Epoch[0/10](47500/50336) || training loss 152.8319 ||\n",
      "Epoch[0/10](48000/50336) || training loss 151.6041 ||\n",
      "Epoch[0/10](48500/50336) || training loss 150.7208 ||\n",
      "Epoch[0/10](49000/50336) || training loss 150.5011 ||\n",
      "Epoch[0/10](49500/50336) || training loss 151.0495 ||\n",
      "Epoch[0/10](50000/50336) || training loss 150.4187 ||\n",
      "Negative Sampling ... Done!!\n",
      "Epoch[1/10](500/50336) || training loss 149.9698 ||\n",
      "Epoch[1/10](1000/50336) || training loss 149.6613 ||\n",
      "Epoch[1/10](1500/50336) || training loss 149.4957 ||\n",
      "Epoch[1/10](2000/50336) || training loss 148.0792 ||\n",
      "Epoch[1/10](2500/50336) || training loss 150.1597 ||\n",
      "Epoch[1/10](3000/50336) || training loss 147.7842 ||\n",
      "Epoch[1/10](3500/50336) || training loss 147.993 ||\n",
      "Epoch[1/10](4000/50336) || training loss 147.3662 ||\n",
      "Epoch[1/10](4500/50336) || training loss 145.6437 ||\n",
      "Epoch[1/10](5000/50336) || training loss 146.4909 ||\n",
      "Epoch[1/10](5500/50336) || training loss 147.0161 ||\n",
      "Epoch[1/10](6000/50336) || training loss 146.6854 ||\n",
      "Epoch[1/10](6500/50336) || training loss 146.0801 ||\n",
      "Epoch[1/10](7000/50336) || training loss 146.0472 ||\n",
      "Epoch[1/10](7500/50336) || training loss 145.0993 ||\n",
      "Epoch[1/10](8000/50336) || training loss 145.3496 ||\n",
      "Epoch[1/10](8500/50336) || training loss 145.2005 ||\n",
      "Epoch[1/10](9000/50336) || training loss 144.8531 ||\n",
      "Epoch[1/10](9500/50336) || training loss 144.0684 ||\n",
      "Epoch[1/10](10000/50336) || training loss 144.8392 ||\n",
      "Epoch[1/10](10500/50336) || training loss 144.4347 ||\n",
      "Epoch[1/10](11000/50336) || training loss 145.1395 ||\n",
      "Epoch[1/10](11500/50336) || training loss 143.9947 ||\n",
      "Epoch[1/10](12000/50336) || training loss 144.1122 ||\n",
      "Epoch[1/10](12500/50336) || training loss 142.5634 ||\n",
      "Epoch[1/10](13000/50336) || training loss 144.7684 ||\n",
      "Epoch[1/10](13500/50336) || training loss 142.4912 ||\n",
      "Epoch[1/10](14000/50336) || training loss 142.2756 ||\n",
      "Epoch[1/10](14500/50336) || training loss 143.103 ||\n",
      "Epoch[1/10](15000/50336) || training loss 142.8157 ||\n",
      "Epoch[1/10](15500/50336) || training loss 142.8669 ||\n",
      "Epoch[1/10](16000/50336) || training loss 142.9206 ||\n",
      "Epoch[1/10](16500/50336) || training loss 142.1011 ||\n",
      "Epoch[1/10](17000/50336) || training loss 142.4841 ||\n",
      "Epoch[1/10](17500/50336) || training loss 141.5969 ||\n",
      "Epoch[1/10](18000/50336) || training loss 142.3119 ||\n",
      "Epoch[1/10](18500/50336) || training loss 142.722 ||\n",
      "Epoch[1/10](19000/50336) || training loss 140.5801 ||\n",
      "Epoch[1/10](19500/50336) || training loss 140.877 ||\n",
      "Epoch[1/10](20000/50336) || training loss 141.3784 ||\n",
      "Epoch[1/10](20500/50336) || training loss 140.4529 ||\n",
      "Epoch[1/10](21000/50336) || training loss 140.744 ||\n",
      "Epoch[1/10](21500/50336) || training loss 140.2686 ||\n",
      "Epoch[1/10](22000/50336) || training loss 140.0533 ||\n",
      "Epoch[1/10](22500/50336) || training loss 140.2922 ||\n",
      "Epoch[1/10](23000/50336) || training loss 140.4532 ||\n",
      "Epoch[1/10](23500/50336) || training loss 139.5393 ||\n",
      "Epoch[1/10](24000/50336) || training loss 138.9706 ||\n",
      "Epoch[1/10](24500/50336) || training loss 140.2947 ||\n",
      "Epoch[1/10](25000/50336) || training loss 138.6946 ||\n",
      "Epoch[1/10](25500/50336) || training loss 139.5404 ||\n",
      "Epoch[1/10](26000/50336) || training loss 139.3041 ||\n",
      "Epoch[1/10](26500/50336) || training loss 138.4074 ||\n",
      "Epoch[1/10](27000/50336) || training loss 139.3357 ||\n",
      "Epoch[1/10](27500/50336) || training loss 139.4753 ||\n",
      "Epoch[1/10](28000/50336) || training loss 138.7474 ||\n",
      "Epoch[1/10](28500/50336) || training loss 138.1496 ||\n",
      "Epoch[1/10](29000/50336) || training loss 138.1564 ||\n",
      "Epoch[1/10](29500/50336) || training loss 138.5532 ||\n",
      "Epoch[1/10](30000/50336) || training loss 136.7215 ||\n",
      "Epoch[1/10](30500/50336) || training loss 138.1924 ||\n",
      "Epoch[1/10](31000/50336) || training loss 137.9162 ||\n",
      "Epoch[1/10](31500/50336) || training loss 138.2097 ||\n",
      "Epoch[1/10](32000/50336) || training loss 137.5724 ||\n",
      "Epoch[1/10](32500/50336) || training loss 137.8138 ||\n",
      "Epoch[1/10](33000/50336) || training loss 137.4138 ||\n",
      "Epoch[1/10](33500/50336) || training loss 137.6807 ||\n",
      "Epoch[1/10](34000/50336) || training loss 137.2933 ||\n",
      "Epoch[1/10](34500/50336) || training loss 137.443 ||\n",
      "Epoch[1/10](35000/50336) || training loss 137.2941 ||\n",
      "Epoch[1/10](35500/50336) || training loss 137.7635 ||\n",
      "Epoch[1/10](36000/50336) || training loss 135.9371 ||\n",
      "Epoch[1/10](36500/50336) || training loss 137.182 ||\n",
      "Epoch[1/10](37000/50336) || training loss 135.8862 ||\n",
      "Epoch[1/10](37500/50336) || training loss 137.1711 ||\n",
      "Epoch[1/10](38000/50336) || training loss 136.3941 ||\n",
      "Epoch[1/10](38500/50336) || training loss 137.5231 ||\n",
      "Epoch[1/10](39000/50336) || training loss 136.0112 ||\n",
      "Epoch[1/10](39500/50336) || training loss 136.717 ||\n",
      "Epoch[1/10](40000/50336) || training loss 136.09 ||\n",
      "Epoch[1/10](40500/50336) || training loss 136.7175 ||\n",
      "Epoch[1/10](41000/50336) || training loss 135.6142 ||\n",
      "Epoch[1/10](41500/50336) || training loss 135.3534 ||\n",
      "Epoch[1/10](42000/50336) || training loss 135.2617 ||\n",
      "Epoch[1/10](42500/50336) || training loss 135.5968 ||\n",
      "Epoch[1/10](43000/50336) || training loss 134.7703 ||\n",
      "Epoch[1/10](43500/50336) || training loss 136.0648 ||\n",
      "Epoch[1/10](44000/50336) || training loss 134.3041 ||\n",
      "Epoch[1/10](44500/50336) || training loss 136.4782 ||\n",
      "Epoch[1/10](45000/50336) || training loss 134.3722 ||\n",
      "Epoch[1/10](45500/50336) || training loss 134.232 ||\n",
      "Epoch[1/10](46000/50336) || training loss 135.2511 ||\n",
      "Epoch[1/10](46500/50336) || training loss 134.9745 ||\n",
      "Epoch[1/10](47000/50336) || training loss 135.5013 ||\n",
      "Epoch[1/10](47500/50336) || training loss 135.1912 ||\n",
      "Epoch[1/10](48000/50336) || training loss 134.5093 ||\n",
      "Epoch[1/10](48500/50336) || training loss 134.4022 ||\n",
      "Epoch[1/10](49000/50336) || training loss 134.2709 ||\n",
      "Epoch[1/10](49500/50336) || training loss 134.2237 ||\n",
      "Epoch[1/10](50000/50336) || training loss 134.3234 ||\n",
      "Negative Sampling ... Done!!\n",
      "Epoch[2/10](500/50336) || training loss 133.2284 ||\n",
      "Epoch[2/10](1000/50336) || training loss 134.1547 ||\n",
      "Epoch[2/10](1500/50336) || training loss 132.3725 ||\n",
      "Epoch[2/10](2000/50336) || training loss 133.4806 ||\n",
      "Epoch[2/10](2500/50336) || training loss 133.5798 ||\n",
      "Epoch[2/10](3000/50336) || training loss 133.1992 ||\n",
      "Epoch[2/10](3500/50336) || training loss 132.9931 ||\n",
      "Epoch[2/10](4000/50336) || training loss 133.1891 ||\n",
      "Epoch[2/10](4500/50336) || training loss 133.5602 ||\n",
      "Epoch[2/10](5000/50336) || training loss 133.5636 ||\n",
      "Epoch[2/10](5500/50336) || training loss 132.3503 ||\n",
      "Epoch[2/10](6000/50336) || training loss 132.6076 ||\n",
      "Epoch[2/10](6500/50336) || training loss 133.5548 ||\n",
      "Epoch[2/10](7000/50336) || training loss 132.77 ||\n",
      "Epoch[2/10](7500/50336) || training loss 132.9702 ||\n",
      "Epoch[2/10](8000/50336) || training loss 131.9308 ||\n",
      "Epoch[2/10](8500/50336) || training loss 133.0448 ||\n",
      "Epoch[2/10](9000/50336) || training loss 133.1782 ||\n",
      "Epoch[2/10](9500/50336) || training loss 132.6509 ||\n",
      "Epoch[2/10](10000/50336) || training loss 132.3108 ||\n",
      "Epoch[2/10](10500/50336) || training loss 132.5467 ||\n",
      "Epoch[2/10](11000/50336) || training loss 134.0848 ||\n",
      "Epoch[2/10](11500/50336) || training loss 131.7224 ||\n",
      "Epoch[2/10](12000/50336) || training loss 131.8469 ||\n",
      "Epoch[2/10](12500/50336) || training loss 132.4188 ||\n",
      "Epoch[2/10](13000/50336) || training loss 131.7508 ||\n",
      "Epoch[2/10](13500/50336) || training loss 131.066 ||\n",
      "Epoch[2/10](14000/50336) || training loss 131.1482 ||\n",
      "Epoch[2/10](14500/50336) || training loss 131.0209 ||\n",
      "Epoch[2/10](15000/50336) || training loss 132.0094 ||\n",
      "Epoch[2/10](15500/50336) || training loss 130.388 ||\n",
      "Epoch[2/10](16000/50336) || training loss 132.5601 ||\n",
      "Epoch[2/10](16500/50336) || training loss 131.1809 ||\n",
      "Epoch[2/10](17000/50336) || training loss 131.7162 ||\n",
      "Epoch[2/10](17500/50336) || training loss 132.2851 ||\n",
      "Epoch[2/10](18000/50336) || training loss 131.7436 ||\n",
      "Epoch[2/10](18500/50336) || training loss 131.5609 ||\n",
      "Epoch[2/10](19000/50336) || training loss 130.4029 ||\n",
      "Epoch[2/10](19500/50336) || training loss 131.2416 ||\n",
      "Epoch[2/10](20000/50336) || training loss 131.0849 ||\n",
      "Epoch[2/10](20500/50336) || training loss 131.8217 ||\n",
      "Epoch[2/10](21000/50336) || training loss 131.4457 ||\n",
      "Epoch[2/10](21500/50336) || training loss 130.2869 ||\n",
      "Epoch[2/10](22000/50336) || training loss 129.7121 ||\n",
      "Epoch[2/10](22500/50336) || training loss 130.1859 ||\n",
      "Epoch[2/10](23000/50336) || training loss 129.1123 ||\n",
      "Epoch[2/10](23500/50336) || training loss 130.0854 ||\n",
      "Epoch[2/10](24000/50336) || training loss 130.6056 ||\n",
      "Epoch[2/10](24500/50336) || training loss 130.8116 ||\n",
      "Epoch[2/10](25000/50336) || training loss 130.1955 ||\n",
      "Epoch[2/10](25500/50336) || training loss 129.4028 ||\n",
      "Epoch[2/10](26000/50336) || training loss 130.796 ||\n",
      "Epoch[2/10](26500/50336) || training loss 130.0138 ||\n",
      "Epoch[2/10](27000/50336) || training loss 130.3227 ||\n",
      "Epoch[2/10](27500/50336) || training loss 131.0361 ||\n",
      "Epoch[2/10](28000/50336) || training loss 129.6284 ||\n",
      "Epoch[2/10](28500/50336) || training loss 130.8727 ||\n",
      "Epoch[2/10](29000/50336) || training loss 129.9946 ||\n",
      "Epoch[2/10](29500/50336) || training loss 130.0979 ||\n",
      "Epoch[2/10](30000/50336) || training loss 130.2616 ||\n",
      "Epoch[2/10](30500/50336) || training loss 130.1061 ||\n",
      "Epoch[2/10](31000/50336) || training loss 129.7105 ||\n",
      "Epoch[2/10](31500/50336) || training loss 129.5614 ||\n",
      "Epoch[2/10](32000/50336) || training loss 129.8295 ||\n",
      "Epoch[2/10](32500/50336) || training loss 129.9013 ||\n",
      "Epoch[2/10](33000/50336) || training loss 128.6413 ||\n",
      "Epoch[2/10](33500/50336) || training loss 129.3726 ||\n",
      "Epoch[2/10](34000/50336) || training loss 129.7685 ||\n",
      "Epoch[2/10](34500/50336) || training loss 129.7257 ||\n",
      "Epoch[2/10](35000/50336) || training loss 130.5708 ||\n",
      "Epoch[2/10](35500/50336) || training loss 129.1349 ||\n",
      "Epoch[2/10](36000/50336) || training loss 129.8517 ||\n",
      "Epoch[2/10](36500/50336) || training loss 129.1342 ||\n",
      "Epoch[2/10](37000/50336) || training loss 129.2791 ||\n",
      "Epoch[2/10](37500/50336) || training loss 128.6606 ||\n",
      "Epoch[2/10](38000/50336) || training loss 128.2838 ||\n",
      "Epoch[2/10](38500/50336) || training loss 129.3001 ||\n",
      "Epoch[2/10](39000/50336) || training loss 128.6359 ||\n",
      "Epoch[2/10](39500/50336) || training loss 128.8583 ||\n",
      "Epoch[2/10](40000/50336) || training loss 128.4599 ||\n",
      "Epoch[2/10](40500/50336) || training loss 128.6352 ||\n",
      "Epoch[2/10](41000/50336) || training loss 128.781 ||\n",
      "Epoch[2/10](41500/50336) || training loss 128.4348 ||\n",
      "Epoch[2/10](42000/50336) || training loss 128.0221 ||\n",
      "Epoch[2/10](42500/50336) || training loss 128.3409 ||\n",
      "Epoch[2/10](43000/50336) || training loss 128.3009 ||\n",
      "Epoch[2/10](43500/50336) || training loss 128.2256 ||\n",
      "Epoch[2/10](44000/50336) || training loss 128.2803 ||\n",
      "Epoch[2/10](44500/50336) || training loss 128.0811 ||\n",
      "Epoch[2/10](45000/50336) || training loss 128.4385 ||\n",
      "Epoch[2/10](45500/50336) || training loss 128.3919 ||\n",
      "Epoch[2/10](46000/50336) || training loss 128.0954 ||\n",
      "Epoch[2/10](46500/50336) || training loss 128.194 ||\n",
      "Epoch[2/10](47000/50336) || training loss 128.4196 ||\n",
      "Epoch[2/10](47500/50336) || training loss 128.7328 ||\n",
      "Epoch[2/10](48000/50336) || training loss 128.2675 ||\n",
      "Epoch[2/10](48500/50336) || training loss 127.9774 ||\n",
      "Epoch[2/10](49000/50336) || training loss 128.3108 ||\n",
      "Epoch[2/10](49500/50336) || training loss 128.0097 ||\n",
      "Epoch[2/10](50000/50336) || training loss 129.5998 ||\n",
      "Negative Sampling ... Done!!\n",
      "Epoch[3/10](500/50336) || training loss 126.702 ||\n",
      "Epoch[3/10](1000/50336) || training loss 127.0011 ||\n",
      "Epoch[3/10](1500/50336) || training loss 127.5726 ||\n",
      "Epoch[3/10](2000/50336) || training loss 126.7427 ||\n",
      "Epoch[3/10](2500/50336) || training loss 127.5569 ||\n",
      "Epoch[3/10](3000/50336) || training loss 127.8635 ||\n",
      "Epoch[3/10](3500/50336) || training loss 127.9525 ||\n",
      "Epoch[3/10](4000/50336) || training loss 127.4497 ||\n",
      "Epoch[3/10](4500/50336) || training loss 125.9782 ||\n",
      "Epoch[3/10](5000/50336) || training loss 126.5857 ||\n",
      "Epoch[3/10](5500/50336) || training loss 126.638 ||\n",
      "Epoch[3/10](6000/50336) || training loss 126.2079 ||\n",
      "Epoch[3/10](6500/50336) || training loss 127.2609 ||\n",
      "Epoch[3/10](7000/50336) || training loss 126.7436 ||\n",
      "Epoch[3/10](7500/50336) || training loss 127.471 ||\n",
      "Epoch[3/10](8000/50336) || training loss 126.5328 ||\n",
      "Epoch[3/10](8500/50336) || training loss 127.1466 ||\n",
      "Epoch[3/10](9000/50336) || training loss 126.3273 ||\n",
      "Epoch[3/10](9500/50336) || training loss 126.7623 ||\n",
      "Epoch[3/10](10000/50336) || training loss 126.3595 ||\n",
      "Epoch[3/10](10500/50336) || training loss 126.5021 ||\n",
      "Epoch[3/10](11000/50336) || training loss 126.736 ||\n",
      "Epoch[3/10](11500/50336) || training loss 126.5484 ||\n",
      "Epoch[3/10](12000/50336) || training loss 125.7962 ||\n",
      "Epoch[3/10](12500/50336) || training loss 126.8062 ||\n",
      "Epoch[3/10](13000/50336) || training loss 126.0995 ||\n",
      "Epoch[3/10](13500/50336) || training loss 127.3801 ||\n",
      "Epoch[3/10](14000/50336) || training loss 126.6981 ||\n",
      "Epoch[3/10](14500/50336) || training loss 126.4377 ||\n",
      "Epoch[3/10](15000/50336) || training loss 126.498 ||\n",
      "Epoch[3/10](15500/50336) || training loss 126.838 ||\n",
      "Epoch[3/10](16000/50336) || training loss 126.7265 ||\n",
      "Epoch[3/10](16500/50336) || training loss 126.8503 ||\n",
      "Epoch[3/10](17000/50336) || training loss 127.4986 ||\n",
      "Epoch[3/10](17500/50336) || training loss 126.1333 ||\n",
      "Epoch[3/10](18000/50336) || training loss 126.1791 ||\n",
      "Epoch[3/10](18500/50336) || training loss 127.5313 ||\n",
      "Epoch[3/10](19000/50336) || training loss 127.9887 ||\n",
      "Epoch[3/10](19500/50336) || training loss 126.551 ||\n",
      "Epoch[3/10](20000/50336) || training loss 127.1857 ||\n",
      "Epoch[3/10](20500/50336) || training loss 126.686 ||\n",
      "Epoch[3/10](21000/50336) || training loss 126.502 ||\n",
      "Epoch[3/10](21500/50336) || training loss 125.6775 ||\n",
      "Epoch[3/10](22000/50336) || training loss 126.757 ||\n",
      "Epoch[3/10](22500/50336) || training loss 126.597 ||\n",
      "Epoch[3/10](23000/50336) || training loss 125.9174 ||\n",
      "Epoch[3/10](23500/50336) || training loss 125.0103 ||\n",
      "Epoch[3/10](24000/50336) || training loss 125.9096 ||\n",
      "Epoch[3/10](24500/50336) || training loss 126.292 ||\n",
      "Epoch[3/10](25000/50336) || training loss 125.7658 ||\n",
      "Epoch[3/10](25500/50336) || training loss 126.4252 ||\n",
      "Epoch[3/10](26000/50336) || training loss 126.5462 ||\n",
      "Epoch[3/10](26500/50336) || training loss 126.5901 ||\n",
      "Epoch[3/10](27000/50336) || training loss 126.1607 ||\n",
      "Epoch[3/10](27500/50336) || training loss 126.8049 ||\n",
      "Epoch[3/10](28000/50336) || training loss 125.6189 ||\n",
      "Epoch[3/10](28500/50336) || training loss 125.1958 ||\n",
      "Epoch[3/10](29000/50336) || training loss 126.3895 ||\n",
      "Epoch[3/10](29500/50336) || training loss 126.0159 ||\n",
      "Epoch[3/10](30000/50336) || training loss 125.9628 ||\n",
      "Epoch[3/10](30500/50336) || training loss 126.0303 ||\n",
      "Epoch[3/10](31000/50336) || training loss 126.0022 ||\n",
      "Epoch[3/10](31500/50336) || training loss 126.035 ||\n",
      "Epoch[3/10](32000/50336) || training loss 126.2578 ||\n",
      "Epoch[3/10](32500/50336) || training loss 125.5614 ||\n",
      "Epoch[3/10](33000/50336) || training loss 125.3603 ||\n",
      "Epoch[3/10](33500/50336) || training loss 126.5195 ||\n",
      "Epoch[3/10](34000/50336) || training loss 126.99 ||\n",
      "Epoch[3/10](34500/50336) || training loss 127.8252 ||\n",
      "Epoch[3/10](35000/50336) || training loss 125.7042 ||\n",
      "Epoch[3/10](35500/50336) || training loss 126.4323 ||\n",
      "Epoch[3/10](36000/50336) || training loss 126.5235 ||\n",
      "Epoch[3/10](36500/50336) || training loss 125.7896 ||\n",
      "Epoch[3/10](37000/50336) || training loss 124.914 ||\n",
      "Epoch[3/10](37500/50336) || training loss 125.2825 ||\n",
      "Epoch[3/10](38000/50336) || training loss 125.5581 ||\n",
      "Epoch[3/10](38500/50336) || training loss 127.0757 ||\n",
      "Epoch[3/10](39000/50336) || training loss 125.8319 ||\n",
      "Epoch[3/10](39500/50336) || training loss 126.2344 ||\n",
      "Epoch[3/10](40000/50336) || training loss 124.9589 ||\n",
      "Epoch[3/10](40500/50336) || training loss 126.0382 ||\n",
      "Epoch[3/10](41000/50336) || training loss 125.4326 ||\n",
      "Epoch[3/10](41500/50336) || training loss 125.2009 ||\n",
      "Epoch[3/10](42000/50336) || training loss 125.0005 ||\n",
      "Epoch[3/10](42500/50336) || training loss 125.542 ||\n",
      "Epoch[3/10](43000/50336) || training loss 125.5133 ||\n",
      "Epoch[3/10](43500/50336) || training loss 126.0337 ||\n",
      "Epoch[3/10](44000/50336) || training loss 125.6482 ||\n",
      "Epoch[3/10](44500/50336) || training loss 125.5449 ||\n",
      "Epoch[3/10](45000/50336) || training loss 125.5198 ||\n",
      "Epoch[3/10](45500/50336) || training loss 126.5637 ||\n",
      "Epoch[3/10](46000/50336) || training loss 125.4048 ||\n",
      "Epoch[3/10](46500/50336) || training loss 124.8447 ||\n",
      "Epoch[3/10](47000/50336) || training loss 125.3956 ||\n",
      "Epoch[3/10](47500/50336) || training loss 124.8043 ||\n",
      "Epoch[3/10](48000/50336) || training loss 124.6181 ||\n",
      "Epoch[3/10](48500/50336) || training loss 126.2386 ||\n",
      "Epoch[3/10](49000/50336) || training loss 125.6068 ||\n",
      "Epoch[3/10](49500/50336) || training loss 125.9817 ||\n",
      "Epoch[3/10](50000/50336) || training loss 124.954 ||\n",
      "Negative Sampling ... Done!!\n",
      "Epoch[4/10](500/50336) || training loss 125.2202 ||\n",
      "Epoch[4/10](1000/50336) || training loss 124.5721 ||\n",
      "Epoch[4/10](1500/50336) || training loss 124.0928 ||\n",
      "Epoch[4/10](2000/50336) || training loss 124.708 ||\n",
      "Epoch[4/10](2500/50336) || training loss 124.5847 ||\n",
      "Epoch[4/10](3000/50336) || training loss 124.0209 ||\n",
      "Epoch[4/10](3500/50336) || training loss 123.6186 ||\n",
      "Epoch[4/10](4000/50336) || training loss 124.615 ||\n",
      "Epoch[4/10](4500/50336) || training loss 123.1401 ||\n",
      "Epoch[4/10](5000/50336) || training loss 125.4823 ||\n",
      "Epoch[4/10](5500/50336) || training loss 123.9602 ||\n",
      "Epoch[4/10](6000/50336) || training loss 124.9807 ||\n",
      "Epoch[4/10](6500/50336) || training loss 124.1278 ||\n",
      "Epoch[4/10](7000/50336) || training loss 124.875 ||\n",
      "Epoch[4/10](7500/50336) || training loss 124.4053 ||\n",
      "Epoch[4/10](8000/50336) || training loss 125.3474 ||\n",
      "Epoch[4/10](8500/50336) || training loss 124.4072 ||\n",
      "Epoch[4/10](9000/50336) || training loss 123.9543 ||\n",
      "Epoch[4/10](9500/50336) || training loss 124.8716 ||\n",
      "Epoch[4/10](10000/50336) || training loss 125.4098 ||\n",
      "Epoch[4/10](10500/50336) || training loss 124.3512 ||\n",
      "Epoch[4/10](11000/50336) || training loss 124.6558 ||\n",
      "Epoch[4/10](11500/50336) || training loss 122.7941 ||\n",
      "Epoch[4/10](12000/50336) || training loss 124.0284 ||\n",
      "Epoch[4/10](12500/50336) || training loss 123.4202 ||\n",
      "Epoch[4/10](13000/50336) || training loss 124.591 ||\n",
      "Epoch[4/10](13500/50336) || training loss 125.651 ||\n",
      "Epoch[4/10](14000/50336) || training loss 125.0752 ||\n",
      "Epoch[4/10](14500/50336) || training loss 124.9255 ||\n",
      "Epoch[4/10](15000/50336) || training loss 124.3844 ||\n",
      "Epoch[4/10](15500/50336) || training loss 123.202 ||\n",
      "Epoch[4/10](16000/50336) || training loss 123.5955 ||\n",
      "Epoch[4/10](16500/50336) || training loss 124.5713 ||\n",
      "Epoch[4/10](17000/50336) || training loss 125.2703 ||\n",
      "Epoch[4/10](17500/50336) || training loss 124.0175 ||\n",
      "Epoch[4/10](18000/50336) || training loss 123.8317 ||\n",
      "Epoch[4/10](18500/50336) || training loss 123.1528 ||\n",
      "Epoch[4/10](19000/50336) || training loss 124.1627 ||\n",
      "Epoch[4/10](19500/50336) || training loss 124.0102 ||\n",
      "Epoch[4/10](20000/50336) || training loss 124.4588 ||\n",
      "Epoch[4/10](20500/50336) || training loss 124.2322 ||\n",
      "Epoch[4/10](21000/50336) || training loss 124.8854 ||\n",
      "Epoch[4/10](21500/50336) || training loss 124.3971 ||\n",
      "Epoch[4/10](22000/50336) || training loss 124.0136 ||\n",
      "Epoch[4/10](22500/50336) || training loss 124.0617 ||\n",
      "Epoch[4/10](23000/50336) || training loss 124.1696 ||\n",
      "Epoch[4/10](23500/50336) || training loss 124.4447 ||\n",
      "Epoch[4/10](24000/50336) || training loss 124.7059 ||\n",
      "Epoch[4/10](24500/50336) || training loss 124.0232 ||\n",
      "Epoch[4/10](25000/50336) || training loss 124.0575 ||\n",
      "Epoch[4/10](25500/50336) || training loss 123.5872 ||\n",
      "Epoch[4/10](26000/50336) || training loss 124.2662 ||\n",
      "Epoch[4/10](26500/50336) || training loss 124.1942 ||\n",
      "Epoch[4/10](27000/50336) || training loss 123.7812 ||\n",
      "Epoch[4/10](27500/50336) || training loss 124.0817 ||\n",
      "Epoch[4/10](28000/50336) || training loss 125.2232 ||\n",
      "Epoch[4/10](28500/50336) || training loss 124.336 ||\n",
      "Epoch[4/10](29000/50336) || training loss 122.907 ||\n",
      "Epoch[4/10](29500/50336) || training loss 124.1224 ||\n",
      "Epoch[4/10](30000/50336) || training loss 124.5551 ||\n",
      "Epoch[4/10](30500/50336) || training loss 125.2075 ||\n",
      "Epoch[4/10](31000/50336) || training loss 123.7712 ||\n",
      "Epoch[4/10](31500/50336) || training loss 125.0979 ||\n",
      "Epoch[4/10](32000/50336) || training loss 124.1093 ||\n",
      "Epoch[4/10](32500/50336) || training loss 122.9849 ||\n",
      "Epoch[4/10](33000/50336) || training loss 124.8376 ||\n",
      "Epoch[4/10](33500/50336) || training loss 124.8567 ||\n",
      "Epoch[4/10](34000/50336) || training loss 124.2849 ||\n",
      "Epoch[4/10](34500/50336) || training loss 123.9298 ||\n",
      "Epoch[4/10](35000/50336) || training loss 124.3545 ||\n",
      "Epoch[4/10](35500/50336) || training loss 124.3623 ||\n",
      "Epoch[4/10](36000/50336) || training loss 124.1211 ||\n",
      "Epoch[4/10](36500/50336) || training loss 124.0568 ||\n",
      "Epoch[4/10](37000/50336) || training loss 123.3764 ||\n",
      "Epoch[4/10](37500/50336) || training loss 123.3935 ||\n",
      "Epoch[4/10](38000/50336) || training loss 124.3182 ||\n",
      "Epoch[4/10](38500/50336) || training loss 124.7015 ||\n",
      "Epoch[4/10](39000/50336) || training loss 124.4003 ||\n",
      "Epoch[4/10](39500/50336) || training loss 123.5717 ||\n",
      "Epoch[4/10](40000/50336) || training loss 123.3349 ||\n",
      "Epoch[4/10](40500/50336) || training loss 125.0043 ||\n",
      "Epoch[4/10](41000/50336) || training loss 123.5475 ||\n",
      "Epoch[4/10](41500/50336) || training loss 123.4591 ||\n",
      "Epoch[4/10](42000/50336) || training loss 123.8195 ||\n",
      "Epoch[4/10](42500/50336) || training loss 123.8325 ||\n",
      "Epoch[4/10](43000/50336) || training loss 124.1713 ||\n",
      "Epoch[4/10](43500/50336) || training loss 123.3949 ||\n",
      "Epoch[4/10](44000/50336) || training loss 124.0401 ||\n",
      "Epoch[4/10](44500/50336) || training loss 123.3493 ||\n",
      "Epoch[4/10](45000/50336) || training loss 125.1861 ||\n",
      "Epoch[4/10](45500/50336) || training loss 123.7753 ||\n",
      "Epoch[4/10](46000/50336) || training loss 123.7062 ||\n",
      "Epoch[4/10](46500/50336) || training loss 122.6733 ||\n",
      "Epoch[4/10](47000/50336) || training loss 123.1403 ||\n",
      "Epoch[4/10](47500/50336) || training loss 124.793 ||\n",
      "Epoch[4/10](48000/50336) || training loss 123.1967 ||\n",
      "Epoch[4/10](48500/50336) || training loss 123.9194 ||\n",
      "Epoch[4/10](49000/50336) || training loss 124.1645 ||\n",
      "Epoch[4/10](49500/50336) || training loss 123.4956 ||\n",
      "Epoch[4/10](50000/50336) || training loss 124.4422 ||\n",
      "Negative Sampling ... Done!!\n",
      "Epoch[5/10](500/50336) || training loss 123.5644 ||\n",
      "Epoch[5/10](1000/50336) || training loss 123.5561 ||\n",
      "Epoch[5/10](1500/50336) || training loss 123.1544 ||\n",
      "Epoch[5/10](2000/50336) || training loss 122.6374 ||\n",
      "Epoch[5/10](2500/50336) || training loss 123.5066 ||\n",
      "Epoch[5/10](3000/50336) || training loss 122.6678 ||\n",
      "Epoch[5/10](3500/50336) || training loss 122.9536 ||\n",
      "Epoch[5/10](4000/50336) || training loss 123.7081 ||\n",
      "Epoch[5/10](4500/50336) || training loss 123.8286 ||\n",
      "Epoch[5/10](5000/50336) || training loss 122.667 ||\n",
      "Epoch[5/10](5500/50336) || training loss 122.9957 ||\n",
      "Epoch[5/10](6000/50336) || training loss 122.1917 ||\n",
      "Epoch[5/10](6500/50336) || training loss 123.3111 ||\n",
      "Epoch[5/10](7000/50336) || training loss 123.282 ||\n",
      "Epoch[5/10](7500/50336) || training loss 122.7901 ||\n",
      "Epoch[5/10](8000/50336) || training loss 123.633 ||\n",
      "Epoch[5/10](8500/50336) || training loss 123.9375 ||\n",
      "Epoch[5/10](9000/50336) || training loss 123.2412 ||\n",
      "Epoch[5/10](9500/50336) || training loss 124.1279 ||\n",
      "Epoch[5/10](10000/50336) || training loss 122.5184 ||\n",
      "Epoch[5/10](10500/50336) || training loss 123.4096 ||\n",
      "Epoch[5/10](11000/50336) || training loss 122.8348 ||\n",
      "Epoch[5/10](11500/50336) || training loss 122.7955 ||\n",
      "Epoch[5/10](12000/50336) || training loss 123.5754 ||\n",
      "Epoch[5/10](12500/50336) || training loss 123.8391 ||\n",
      "Epoch[5/10](13000/50336) || training loss 122.4274 ||\n",
      "Epoch[5/10](13500/50336) || training loss 122.8112 ||\n",
      "Epoch[5/10](14000/50336) || training loss 122.2361 ||\n",
      "Epoch[5/10](14500/50336) || training loss 122.79 ||\n",
      "Epoch[5/10](15000/50336) || training loss 122.8932 ||\n",
      "Epoch[5/10](15500/50336) || training loss 123.2884 ||\n",
      "Epoch[5/10](16000/50336) || training loss 124.2466 ||\n",
      "Epoch[5/10](16500/50336) || training loss 124.3927 ||\n",
      "Epoch[5/10](17000/50336) || training loss 124.1745 ||\n",
      "Epoch[5/10](17500/50336) || training loss 123.4567 ||\n",
      "Epoch[5/10](18000/50336) || training loss 123.3918 ||\n",
      "Epoch[5/10](18500/50336) || training loss 123.41 ||\n",
      "Epoch[5/10](19000/50336) || training loss 123.5349 ||\n",
      "Epoch[5/10](19500/50336) || training loss 123.4614 ||\n",
      "Epoch[5/10](20000/50336) || training loss 123.3232 ||\n",
      "Epoch[5/10](20500/50336) || training loss 123.5046 ||\n",
      "Epoch[5/10](21000/50336) || training loss 123.2612 ||\n",
      "Epoch[5/10](21500/50336) || training loss 122.4722 ||\n",
      "Epoch[5/10](22000/50336) || training loss 122.9996 ||\n",
      "Epoch[5/10](22500/50336) || training loss 123.5653 ||\n",
      "Epoch[5/10](23000/50336) || training loss 123.3967 ||\n",
      "Epoch[5/10](23500/50336) || training loss 123.8113 ||\n",
      "Epoch[5/10](24000/50336) || training loss 123.1598 ||\n",
      "Epoch[5/10](24500/50336) || training loss 123.3868 ||\n",
      "Epoch[5/10](25000/50336) || training loss 123.3098 ||\n",
      "Epoch[5/10](25500/50336) || training loss 122.6584 ||\n",
      "Epoch[5/10](26000/50336) || training loss 122.4558 ||\n",
      "Epoch[5/10](26500/50336) || training loss 123.4375 ||\n",
      "Epoch[5/10](27000/50336) || training loss 122.9309 ||\n",
      "Epoch[5/10](27500/50336) || training loss 122.2789 ||\n",
      "Epoch[5/10](28000/50336) || training loss 122.1329 ||\n",
      "Epoch[5/10](28500/50336) || training loss 123.0244 ||\n",
      "Epoch[5/10](29000/50336) || training loss 124.5252 ||\n",
      "Epoch[5/10](29500/50336) || training loss 123.2154 ||\n",
      "Epoch[5/10](30000/50336) || training loss 122.5809 ||\n",
      "Epoch[5/10](30500/50336) || training loss 122.2554 ||\n",
      "Epoch[5/10](31000/50336) || training loss 123.4587 ||\n",
      "Epoch[5/10](31500/50336) || training loss 123.6712 ||\n",
      "Epoch[5/10](32000/50336) || training loss 123.1634 ||\n",
      "Epoch[5/10](32500/50336) || training loss 122.3896 ||\n",
      "Epoch[5/10](33000/50336) || training loss 123.1145 ||\n",
      "Epoch[5/10](33500/50336) || training loss 122.8151 ||\n",
      "Epoch[5/10](34000/50336) || training loss 122.8691 ||\n",
      "Epoch[5/10](34500/50336) || training loss 122.3194 ||\n",
      "Epoch[5/10](35000/50336) || training loss 123.8985 ||\n",
      "Epoch[5/10](35500/50336) || training loss 122.8939 ||\n",
      "Epoch[5/10](36000/50336) || training loss 123.2795 ||\n",
      "Epoch[5/10](36500/50336) || training loss 122.8767 ||\n",
      "Epoch[5/10](37000/50336) || training loss 124.6071 ||\n",
      "Epoch[5/10](37500/50336) || training loss 123.1649 ||\n",
      "Epoch[5/10](38000/50336) || training loss 121.952 ||\n",
      "Epoch[5/10](38500/50336) || training loss 123.7117 ||\n",
      "Epoch[5/10](39000/50336) || training loss 123.6515 ||\n",
      "Epoch[5/10](39500/50336) || training loss 122.2545 ||\n",
      "Epoch[5/10](40000/50336) || training loss 121.9356 ||\n",
      "Epoch[5/10](40500/50336) || training loss 122.9583 ||\n",
      "Epoch[5/10](41000/50336) || training loss 122.9365 ||\n",
      "Epoch[5/10](41500/50336) || training loss 122.8915 ||\n",
      "Epoch[5/10](42000/50336) || training loss 122.7905 ||\n",
      "Epoch[5/10](42500/50336) || training loss 123.9165 ||\n",
      "Epoch[5/10](43000/50336) || training loss 123.9571 ||\n",
      "Epoch[5/10](43500/50336) || training loss 123.4143 ||\n",
      "Epoch[5/10](44000/50336) || training loss 122.5472 ||\n",
      "Epoch[5/10](44500/50336) || training loss 123.2389 ||\n",
      "Epoch[5/10](45000/50336) || training loss 123.2898 ||\n",
      "Epoch[5/10](45500/50336) || training loss 124.4666 ||\n",
      "Epoch[5/10](46000/50336) || training loss 122.355 ||\n",
      "Epoch[5/10](46500/50336) || training loss 123.4849 ||\n",
      "Epoch[5/10](47000/50336) || training loss 123.5619 ||\n",
      "Epoch[5/10](47500/50336) || training loss 122.8344 ||\n",
      "Epoch[5/10](48000/50336) || training loss 123.5011 ||\n",
      "Epoch[5/10](48500/50336) || training loss 123.3697 ||\n",
      "Epoch[5/10](49000/50336) || training loss 122.9036 ||\n",
      "Epoch[5/10](49500/50336) || training loss 122.1492 ||\n",
      "Epoch[5/10](50000/50336) || training loss 123.2317 ||\n",
      "Negative Sampling ... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/movie-recommendation/notebooks/model/Factorizing Personalized Marcov Chain.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m loss_value \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNegative Sampling ...\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m train_loader\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mnegative_sampling()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000008vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone!!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n",
      "\u001b[1;32m/opt/ml/movie-recommendation/notebooks/model/Factorizing Personalized Marcov Chain.ipynb Cell 5'\u001b[0m in \u001b[0;36mSequentialDataset.negative_sampling\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000004vscode-remote?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m u, i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mvalues:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000004vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_negative):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000004vscode-remote?line=23'>24</a>\u001b[0m         j \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_item)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000004vscode-remote?line=24'>25</a>\u001b[0m         \u001b[39mwhile\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser2seq[u]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.219.178/opt/ml/movie-recommendation/notebooks/model/Factorizing%20Personalized%20Marcov%20Chain.ipynb#ch0000004vscode-remote?line=25'>26</a>\u001b[0m             j \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_item)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "log_interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\tloss_value = 0\n",
    "\n",
    "\tprint('Negative Sampling ...', end=' ')\n",
    "\ttrain_loader.dataset.negative_sampling()\n",
    "\tprint('Done!!')\n",
    "\n",
    "\tfor idx, batch in enumerate(train_loader):\n",
    "\t\tuser, pos_item, neg_item, item_seq, seq_len =\\\n",
    "    \t\t(v.to(device) for _,v in batch.items())\n",
    "\n",
    "\t\tmodel.zero_grad()\n",
    "\t\tprediction_i = model(user, pos_item, item_seq, seq_len)\n",
    "\t\tprediction_j = model(user, neg_item, item_seq, seq_len)\n",
    "\t\tloss =- (prediction_i - prediction_j).sigmoid().log().sum()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tloss_value += loss.item()\n",
    "\n",
    "\t\tif (idx + 1) % log_interval == 0:\n",
    "\t\t\ttrain_loss = loss_value / log_interval\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf\"Epoch[{epoch}/{epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "\t\t\t\tf\"training loss {train_loss:4.7} ||\"\n",
    "\t\t\t)\n",
    "\t\t\tloss_value = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SequentialDataset(df)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False, \n",
    "    drop_last=False,\n",
    "    collate_fn=make_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FPMC(\n",
    "    user_num = train_loader.dataset.n_user, \n",
    "    item_num = train_loader.dataset.n_item,\n",
    "    factor_num = 10, \n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./best.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inference results... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22771/31360 [03:31<01:14, 114.93it/s]"
     ]
    }
   ],
   "source": [
    "sub_u = []\n",
    "sub_i = []\n",
    "\n",
    "print(\"Calculating inference results...\", end=' ')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in tqdm(test_loader):\n",
    "        user, pos_item, neg_item, item_seq, seq_len =\\\n",
    "            (v.to(device) for _,v in batch.items())\n",
    "\n",
    "        prediction = model.predict(user, pos_item, item_seq, seq_len).squeeze()\n",
    "\n",
    "        ranking = torch.topk(prediction, len(prediction))[1]\n",
    "        \n",
    "        pred = []\n",
    "        for item_id in ranking :\n",
    "            if item_id in item_seq :\n",
    "                continue\n",
    "            u = id_to_user[int(user)]\n",
    "            i = id_to_movie[int(item_id)]\n",
    "            sub_u.append(u)\n",
    "            sub_i.append(i)\n",
    "            pred.append(i)\n",
    "            if len(pred) == 10 :\n",
    "                break\n",
    "\n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = {\"user\" : sub_u, \"item\" : sub_i}\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv(f'/opt/ml/movie-recommendation/BPR/output/fpmc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDatasetv2(Dataset):\n",
    "    def __init__(self, data, num_negative=10, window_size=10, is_training=False) :\n",
    "        self.data = data[['user', 'item']]\n",
    "        self.n_user = self.data['user'].nunique() \n",
    "        self.n_item = self.data['item'].nunique()\n",
    "        self.num_negative = num_negative\n",
    "        self.is_training = is_training\n",
    "        self.window_size = window_size//2\n",
    "        \n",
    "        self.user2seq = dict()\n",
    "        user_item_sequence = list(self.data.groupby(by='user')['item'])\n",
    "        for user, item_seq in user_item_sequence :\n",
    "            self.user2seq[user] = list(item_seq)\n",
    "        \n",
    "        if not self.is_training :\n",
    "            self.user_id = list(self.user2seq.keys())[0]\n",
    "            self.item_seq = list(self.user2seq.values())[0]\n",
    "\n",
    "    def negative_sampling(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "        negative_samples = []\n",
    "        \n",
    "        for u, i in self.data.values:\n",
    "            for _ in range(self.num_negative):\n",
    "                j = np.random.randint(self.n_item)\n",
    "                while j in self.user2seq[u]:\n",
    "                    j = np.random.randint(self.n_item)\n",
    "                negative_samples.append([u, i, j])\n",
    "        self.features = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_negative * len(self.data) if self.is_training else len(self.item_seq)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_training :\n",
    "            user = self.features[idx][0]\n",
    "            pos_item = self.features[idx][1]\n",
    "            neg_item = self.features[idx][2]\n",
    "            user_seq = self.user2seq[user]\n",
    "            item_seq = list()\n",
    "\n",
    "            index = self.find_index(user_seq, pos_item)\n",
    "            \n",
    "            item_seq.extend(user_seq[index-self.window_size:index] if index >= self.window_size else user_seq[0:index])\n",
    "            item_seq.extend(user_seq[index+1 : index+1+self.window_size])\n",
    "\n",
    "            return {\"user\":torch.tensor(user),\n",
    "                    \"pos_item\": torch.tensor(pos_item),\n",
    "                    \"neg_item\": torch.tensor(neg_item),\n",
    "                    \"item_seq\": torch.tensor(item_seq),\n",
    "                    \"seq_len\": torch.tensor(len(item_seq)),}\n",
    "\n",
    "        item_seq = self.item_seq[idx-self.window_size:idx+1+self.window_size] if idx >= self.window_size \\\n",
    "                else self.item_seq[0:idx+1+self.window_size]\n",
    "\n",
    "        return {\"user\":torch.tensor(self.user_id),\n",
    "                \"pos_item\": torch.arange(0,6807),\n",
    "                \"neg_item\": torch.tensor([0]),\n",
    "                \"item_seq\": torch.tensor(item_seq),\n",
    "                \"seq_len\": torch.tensor(len(item_seq)),}\n",
    "    \n",
    "    def find_index(self, seq, item):\n",
    "        index = 0 \n",
    "        for i in seq:\n",
    "            if i == item:\n",
    "                break\n",
    "            index += 1\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDatasetv3(Dataset):\n",
    "    def __init__(self, data, num_negative=10, is_training=False) :\n",
    "        self.data = data[['user', 'item']]\n",
    "        self.n_user = self.data['user'].nunique() \n",
    "        self.n_item = self.data['item'].nunique()\n",
    "        self.num_negative = num_negative\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.user2seq = dict()\n",
    "        user_item_sequence = list(self.data.groupby(by='user')['item'])\n",
    "        for user, item_seq in user_item_sequence :\n",
    "            self.user2seq[user] = list(item_seq)\n",
    "        \n",
    "        if not self.is_training :\n",
    "            self.user = list(self.user2seq.keys())[0]\n",
    "            self.item_seq = list(self.user2seq.values())[0]\n",
    "\n",
    "    def negative_sampling(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "        negative_samples = []\n",
    "        \n",
    "        for u, i in self.data.values:\n",
    "            for _ in range(self.num_negative):\n",
    "                j = np.random.randint(self.n_item)\n",
    "                while j in self.user2seq[u]:\n",
    "                    j = np.random.randint(self.n_item)\n",
    "                negative_samples.append([u, i, j])\n",
    "        self.features = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_negative * len(self.data) if self.is_training else len(self.item_seq)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_training :\n",
    "            user = self.features[idx][0]\n",
    "            pos_item = self.features[idx][1]\n",
    "            neg_item = self.features[idx][2]\n",
    "            user_seq = self.user2seq[user]\n",
    "            item_seq = list()\n",
    "\n",
    "            for item in user_seq :\n",
    "                if item == pos_item :\n",
    "                    break\n",
    "                item_seq.append(item)\n",
    "            \n",
    "            if len(item_seq) == 0 :\n",
    "                item_seq.append(user_seq[1])\n",
    "        \n",
    "        return {\"user\":torch.tensor(user),\n",
    "                \"pos_item\": torch.tensor(pos_item),\n",
    "                \"neg_item\": torch.tensor(neg_item),\n",
    "                \"item_seq\": torch.tensor(item_seq),\n",
    "                \"seq_len\": torch.tensor(len(item_seq)),}\\\n",
    "                if self.is_training else \\\n",
    "                {\"user\":torch.tensor(self.user),\n",
    "                \"pos_item\": torch.arange(0,6807),\n",
    "                \"neg_item\": torch.tensor([0]),\n",
    "                \"item_seq\": torch.tensor(self.item_seq[:idx+1] if idx>0 else self.item_seq[:2]),\n",
    "                \"seq_len\": torch.tensor(idx+1 if idx>0 else 2),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/movie-recommendation/data/train/'\n",
    "\n",
    "train_dataset = SequentialDatasetv3(df, num_negative=10, is_training=True)\n",
    "user_item_dfs = df.groupby('user')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FPMC(\n",
    "    user_num = train_dataset.n_user, \n",
    "    item_num = train_dataset.n_item,\n",
    "    factor_num = 32, \n",
    ").to(device)\n",
    "\n",
    "model_dir = '/opt/ml/movie-recommendation/BPR/model/exp/epoch-9.pth'\n",
    "model.load_state_dict(torch.load(model_dir, map_location=device))\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inference results... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31360/31360 [1:16:03<00:00,  6.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sub_u = []\n",
    "sub_i = []\n",
    "\n",
    "print(\"Calculating inference results...\", end=' ')\n",
    "\n",
    "for user_id, item_df in tqdm(user_item_dfs):\n",
    "    test_dataset = SequentialDatasetv2(item_df)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False, \n",
    "        drop_last=False,\n",
    "        collate_fn=make_batch,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        prediction = torch.zeros(6807).to(device)\n",
    "\n",
    "        for batch in test_loader:\n",
    "            user, pos_item, neg_item, item_seq, seq_len =\\\n",
    "                (v.to(device) for _,v in batch.items())\n",
    "\n",
    "            output = model.predict(user, pos_item, item_seq, seq_len)\n",
    "            prediction += output.sum(axis=0)\n",
    "\n",
    "        ranking = torch.topk(prediction, len(prediction))[1]\n",
    "    \n",
    "    pred = []\n",
    "    for item_id in ranking :\n",
    "        if item_id in test_dataset.item_seq :\n",
    "            continue\n",
    "        u = id_to_user[int(user_id)]\n",
    "        i = id_to_movie[int(item_id)]\n",
    "        sub_u.append(u)\n",
    "        sub_i.append(i)\n",
    "        pred.append(i)\n",
    "        if len(pred) == 10 :\n",
    "            break\n",
    "\n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = {\"user\" : sub_u, \"item\" : sub_i}\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv(f'/opt/ml/movie-recommendation/BPR/output/fpmcV3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
