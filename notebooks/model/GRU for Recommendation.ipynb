{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset(object):\n",
    "    def __init__(self, data_path, session_key='session', item_key='item', time_key='time'):\n",
    "        # Read csv\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        \n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "\n",
    "        self.add_item_indices()\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def add_item_indices(self):\n",
    "        with open('/opt/ml/movie-recommendation/data/train/zero_mapping.json', 'r') as f:\n",
    "            dict_data= json.load(f)\n",
    "        self.df['item']  = self.df['item'].map(lambda x : dict_data['item'][str(x)])\n",
    "\n",
    "    def get_click_offset(self):\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.df[self.item_key].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataLoader():\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item.values[start]\n",
    "\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item.values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU4REC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=100, num_layers=3, final_act='tanh',\n",
    "                 dropout_hidden=0.5, dropout_input=0.5, batch_size=50, embedding_dim=-1, use_cuda=True):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = self.input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_input = dropout_input\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.onehot_buffer = self.init_emb()\n",
    "        self.h2o = nn.Linear(hidden_size, self.output_size)\n",
    "        self.create_final_activation(final_act)\n",
    "        if self.embedding_dim != -1:\n",
    "            self.look_up = nn.Embedding(input_size, self.embedding_dim)\n",
    "            self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        else:\n",
    "            self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def create_final_activation(self, final_act):\n",
    "        if final_act == 'tanh':\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_act == 'relu':\n",
    "            self.final_activation = nn.ReLU()\n",
    "        elif final_act == 'softmax':\n",
    "            self.final_activation = nn.Softmax()\n",
    "        elif final_act == 'softmax_logit':\n",
    "            self.final_activation = nn.LogSoftmax()\n",
    "        elif final_act.startswith('elu-'):\n",
    "            self.final_activation = nn.ELU(alpha=float(final_act.split('-')[1]))\n",
    "        elif final_act.startswith('leaky-'):\n",
    "            self.final_activation = nn.LeakyReLU(negative_slope=float(final_act.split('-')[1]))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input (B,): a batch of item indices from a session-parallel mini-batch.\n",
    "            target (B,): torch.LongTensor of next item indices from a session-parallel mini-batch.\n",
    "        Returns:\n",
    "            logit (B,C): Variable that stores the logits for the next items in the session-parallel mini-batch\n",
    "            hidden: GRU hidden state\n",
    "        '''\n",
    "\n",
    "        if self.embedding_dim == -1:\n",
    "            embedded = self.onehot_encode(input)\n",
    "            if self.training and self.dropout_input > 0: embedded = self.embedding_dropout(embedded)\n",
    "            embedded = embedded.unsqueeze(0)\n",
    "        else:\n",
    "            embedded = input.unsqueeze(0)\n",
    "            embedded = self.look_up(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
    "        output = output.view(-1, output.size(-1))  #(B,H)\n",
    "        logit = self.final_activation(self.h2o(output))\n",
    "\n",
    "        return logit, hidden\n",
    "\n",
    "    def init_emb(self):\n",
    "        '''\n",
    "        Initialize the one_hot embedding buffer, which will be used for producing the one-hot embeddings efficiently\n",
    "        '''\n",
    "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
    "        onehot_buffer = onehot_buffer.to(self.device)\n",
    "        return onehot_buffer\n",
    "\n",
    "    def onehot_encode(self, input):\n",
    "        \"\"\"\n",
    "        Returns a one-hot vector corresponding to the input\n",
    "        Args:\n",
    "            input (B,): torch.LongTensor of item indices\n",
    "            buffer (B,output_size): buffer that stores the one-hot vector\n",
    "        Returns:\n",
    "            one_hot (B,C): torch.FloatTensor of one-hot vectors\n",
    "        \"\"\"\n",
    "        self.onehot_buffer.zero_()\n",
    "        index = input.view(-1, 1)\n",
    "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def embedding_dropout(self, input):\n",
    "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.dropout_input)\n",
    "        mask = torch.bernoulli(p_drop).expand_as(input) / (1 - self.dropout_input)\n",
    "        mask = mask.to(self.device)\n",
    "        input = input * mask\n",
    "        return input\n",
    "\n",
    "    def init_hidden(self):\n",
    "        '''\n",
    "        Initialize the hidden state of the GRU\n",
    "        '''\n",
    "        try:\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        except:\n",
    "            self.device = 'cpu'\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOP1_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1_max, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_hidden(hidden, mask):\n",
    "    \"\"\"Helper function that resets hidden state when some sessions terminate\"\"\"\n",
    "    if len(mask) != 0:\n",
    "        hidden[:, mask, :] = 0\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(indices, targets, k=10): #recall --> wether next item in session is within top K=20 recommended items or not\n",
    "    \"\"\"\n",
    "    Calculates the recall score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "    \"\"\"\n",
    "    _, indices = torch.topk(indices, k, -1)\n",
    "    targets = targets.view(-1, 1).expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    if len(hits) == 0:\n",
    "        return 0\n",
    "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
    "    recall = float(n_hits) / targets.size(0)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/movie-recommendation/data/train/'\n",
    "eval_dir = '/opt/ml/movie-recommendation/data/eval/'\n",
    "\n",
    "train_dataset = SessionDataset(train_dir+'gru/train.csv')\n",
    "valid_dataset = SessionDataset(train_dir+'gru/valid.csv')\n",
    "test_dataset = SessionDataset(train_dir+'gru/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = SessionDataLoader(train_dataset)\n",
    "valid_loader = SessionDataLoader(valid_dataset)\n",
    "test_loader = SessionDataLoader(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([5775, 5238,  406, 1994, 6585, 4862, 3021, 4755, 4432, 5315, 5765, 6559,\n",
      "        6458, 4390, 5807, 6539, 4257, 2669,  903, 2635, 5899,  758, 1583, 2204,\n",
      "        5899, 4729, 1645, 5828,  212, 5209, 5177, 5507,  167, 6695, 4982, 5246,\n",
      "        4237, 5246, 4711,  212, 3728, 5595, 6452, 5280, 4242, 4157,  756,  101,\n",
      "        5765, 4512])\n",
      "==========Test Input==========\n",
      "tensor([5775, 5238,  406, 1994, 6585, 4862, 3021, 4755, 4432, 5315, 5765, 6559,\n",
      "        6458, 4390, 5807, 6539, 4257, 2669,  903, 2635, 5899,  758, 1583, 2204,\n",
      "        5899, 4729, 1645, 5828,  212, 5209, 5177, 5507,  167, 6695, 4982, 5246,\n",
      "        4237, 5246, 4711,  212, 3728, 5595, 6452, 5280, 4242, 4157,  756,  101,\n",
      "        5765, 4512])\n",
      "==========Next Input==========\n",
      "tensor([5703, 3317,  903, 1067, 3849, 5879,  828,  817,   62, 4952, 6063, 6619,\n",
      "        1120, 6700, 4951, 6437, 3727, 2140,  774, 4582, 3070,  218, 1937,  714,\n",
      "        1034, 4432, 6079, 5886, 3506, 5401, 5184, 5620, 5974,    3, 5024, 4993,\n",
      "        1092, 1468, 5315,  801, 5588, 1730, 6251, 2058, 4237, 5885,  773, 3852,\n",
      "        5761, 1716])\n",
      "==========Mask==========\n",
      "[] []\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([5703, 3317,  903, 1067, 3849, 5691,  828,  817,   62, 4952, 6063, 6619,\n",
      "        1120, 6700, 3552, 5970, 3727, 2140,  774, 4582,  789,  218, 1937,  714,\n",
      "        1034, 4432, 6079, 5886, 3506, 5401, 5184, 5620, 5974,    3, 5024,  340,\n",
      "        1092, 1468, 5315,  801, 5588, 1730, 6251, 2058, 1707,  426,  773, 3852,\n",
      "         219, 1716])\n",
      "==========Test Input==========\n",
      "tensor([5703, 3317,  903, 1067, 3849, 5879,  828,  817,   62, 4952, 6063, 6619,\n",
      "        1120, 6700, 4951, 6437, 3727, 2140,  774, 4582, 3070,  218, 1937,  714,\n",
      "        1034, 4432, 6079, 5886, 3506, 5401, 5184, 5620, 5974,    3, 5024, 4993,\n",
      "        1092, 1468, 5315,  801, 5588, 1730, 6251, 2058, 4237, 5885,  773, 3852,\n",
      "        5761, 1716])\n",
      "==========Next Input==========\n",
      "tensor([5765, 3497,  902, 6713, 4710, 4320,  816,  107, 6345, 6200, 5811, 6513,\n",
      "        3819, 6029, 5997, 6324, 4119,  148, 1070, 2326,  302, 1625, 1212,  844,\n",
      "        5926, 2973, 5727,  165, 1718, 5237, 4864, 5505, 5226, 6401, 6789,  816,\n",
      "        3453, 6585,  403,  346, 3192, 2085, 3922, 6520, 1411,  320,  760, 2439,\n",
      "         319, 6035])\n",
      "==========Mask==========\n",
      "[ 5 14 15 20 35 44 45 48] []\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([5765, 3497,  902, 6713, 4710, 4320,  816,  107, 6345, 6200, 5811, 6513,\n",
      "        3819, 6029, 5997, 4556, 4119,  148, 1070, 2999,  302, 1625, 1212,  844,\n",
      "        5926, 2973, 5727,  165, 1718, 5237, 4864, 5505, 1438, 6401, 6789,  816,\n",
      "        3453, 6585,  403,  346, 6051, 2085, 3922, 6520, 1411,  320,  760, 2439,\n",
      "         319, 6035])\n",
      "==========Test Input==========\n",
      "tensor([5765, 3497,  902, 6713, 4710, 5691,  816,  107, 6345, 6200, 5811, 6513,\n",
      "        3819, 6029, 3552, 5970, 4119,  148, 1070, 2326,  789, 1625, 1212,  844,\n",
      "        5926, 2973, 5727,  165, 1718, 5237, 4864, 5505, 5226, 6401, 6789,  340,\n",
      "        3453, 6585,  403,  346, 3192, 2085, 3922, 6520, 1707,  426,  760, 2439,\n",
      "         219, 6035])\n",
      "==========Next Input==========\n",
      "tensor([5725,  729, 1371, 5346, 6374,  814,  756, 6374, 2251, 3287, 5788, 5554,\n",
      "        5177, 6709, 1771,  538, 4293,  366,  819, 3950,  810,  360, 1874, 1208,\n",
      "        3007, 5730, 2630, 5260, 5167, 4650, 4211, 3661,  570, 2581, 4839, 1334,\n",
      "         309, 1263, 5036, 2736, 5894, 3160, 6279, 6442, 2126, 1583,  798, 3458,\n",
      "        1061, 2516])\n",
      "==========Mask==========\n",
      "[15 19 32 40] [ 5 14 15 20 35 44 45 48]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([5725,  729, 1371, 5346, 6374,  814,  756, 6374, 2251, 3287, 5788,  758,\n",
      "        5177, 6709, 1771,  538, 4293,  366,  819, 3950,  810,  360, 1874, 1208,\n",
      "        3007, 5730, 6645, 3571, 5167, 4650, 4211, 3661,  570, 2581, 4839, 1334,\n",
      "         309, 4755, 5036, 2736, 5894, 3160, 1845, 6442, 2126, 1583,  798,  359,\n",
      "        1061, 2516])\n",
      "==========Test Input==========\n",
      "tensor([5725,  729, 1371, 5346, 6374, 4320,  756, 6374, 2251, 3287, 5788, 5554,\n",
      "        5177, 6709, 5997, 6324, 4293,  366,  819, 4556,  302,  360, 1874, 1208,\n",
      "        3007, 5730, 2630, 5260, 5167, 4650, 4211, 3661, 2999, 2581, 4839,  816,\n",
      "         309, 1263, 5036, 2736, 1438, 3160, 6279, 6442, 1411,  320,  798, 3458,\n",
      "         319, 2516])\n",
      "==========Next Input==========\n",
      "tensor([5718, 3453, 2039, 5560, 6587, 6158,  725,  757, 4626, 5055, 5802, 5718,\n",
      "        1652, 4114, 6452, 6762,  121, 2376,  945, 3497, 1685,  657, 2243, 1563,\n",
      "        1878, 5809, 2216, 5669, 3819, 5267, 5993, 6585, 1856, 2999, 4808, 3473,\n",
      "        3826, 1258, 3711, 2837, 4710,  981, 2246, 6497, 1504, 3513,   31, 4772,\n",
      "         986,  112])\n",
      "==========Mask==========\n",
      "[11 26 27 37 42 47] [19 32 40]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([5718, 3453, 2039, 5560, 6587, 3036,  725,  757, 4626, 5055, 5802, 5718,\n",
      "        1652, 3098, 6452, 6762, 2276, 2376,  945, 3497, 1685,  657, 2243, 1563,\n",
      "         827, 5809, 2216, 5669, 3473, 5267, 5993,  215, 1856, 2999, 4808, 3473,\n",
      "        3826, 1258, 3711, 2837, 4710,  981, 2246, 2423, 1504, 3513,   31, 4772,\n",
      "         986,  112])\n",
      "==========Test Input==========\n",
      "tensor([5718, 3453, 2039, 5560, 6587,  814,  725,  757, 4626, 5055, 5802, 6051,\n",
      "        1652, 4114, 1771,  758,  121, 2376,  945,  538,  810,  657, 2243, 1563,\n",
      "        1878, 5809, 6645, 3571, 3819, 5267, 5993, 6585, 3950, 2999, 4808, 1334,\n",
      "        3826, 4755, 3711, 2837,  570,  981, 1845, 6497, 2126, 1583,   31,  359,\n",
      "        1061,  112])\n",
      "==========Next Input==========\n",
      "tensor([5696, 4857,  898, 2029, 4171, 2201,  707, 2474, 4573,  908, 5710, 1030,\n",
      "        5040, 4759, 6434, 6245, 1452,  651, 1451, 6596, 3065, 1334, 1871,  883,\n",
      "        1940, 4697, 5720, 4578,  810, 4495, 1608,  871, 1883, 6729, 4802,  564,\n",
      "        1691,  756, 3570,  837,  981, 4950, 1307, 4854,   60,  538, 1701, 5246,\n",
      "         506,  779])\n",
      "==========Mask==========\n",
      "[ 5 13 16 24 28 31 43] [11 15 26 27 37 42 47]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([5696, 4857,  898, 5919, 4171, 2201,  707, 2474, 4573,  908, 6743, 1030,\n",
      "        5040, 4759,  236, 6245, 1452,  651, 1451, 6596, 3065, 1334, 1871,  883,\n",
      "        1940, 4697, 5720, 4578,  810, 4495, 1608,  871, 1883, 6729, 4802,  564,\n",
      "        1691,  756, 3570,  837,  981, 4950, 1307, 4854,   60,  538, 1701, 5246,\n",
      "         506,  779])\n",
      "==========Test Input==========\n",
      "tensor([5696, 4857,  898, 2029, 4171, 6158,  707, 2474, 4573,  908, 5710, 5894,\n",
      "        5040, 3036, 6452, 5718, 3098,  651, 1451, 6762, 1685, 1334, 1871,  883,\n",
      "        2276, 4697, 2216, 5669,  827, 4495, 1608, 3473, 3497, 6729, 4802, 3473,\n",
      "        1691, 1258, 3570,  837, 1856, 4950, 2246,  215, 1504, 3513, 1701, 4772,\n",
      "         986,  779])\n",
      "==========Next Input==========\n",
      "tensor([4936, 1465,   31, 5238, 1639, 5384,  657,  574, 2942,  180, 5076, 2543,\n",
      "        5275, 1583, 2599,  925, 2216, 1363, 1118, 3238,  808, 3728, 2313, 1073,\n",
      "        6413, 3664, 1990, 4265,  235, 4426, 4470, 1267, 1907, 2379, 3228,   90,\n",
      "        2423,  688, 1034, 5087, 1695, 3984,  171,   35, 5475,  780,  475, 2385,\n",
      "        1249, 1081])\n",
      "==========Mask==========\n",
      "[ 3 10 14] [13 16 24 28 31 43]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([4936, 1465,   31, 2952, 1639, 5384,  657,  574, 2942,  180, 5076, 2581,\n",
      "        5275, 1583, 2599,  925, 2216, 1363, 1118, 2320,  808, 3728, 2313, 1073,\n",
      "        6413, 3664, 1990, 4265,  235, 4426, 4470, 1267, 1907, 2379, 3228,   90,\n",
      "        5420,  688, 1034, 5087, 1695, 3984,  171,   35, 5475,  780,  475, 2385,\n",
      "        1249, 1081])\n",
      "==========Test Input==========\n",
      "tensor([4936, 1465,   31, 2423, 1639, 5919,  657,  574, 2942,  180, 6743, 4710,\n",
      "        5275, 2201, 6434, 1030, 4759, 1363, 1118, 6245, 3065, 3728, 2313, 1073,\n",
      "        1452, 3664, 5720, 4578, 1940, 4426, 4470,  810, 6596, 2379, 3228,  564,\n",
      "        2423,  756, 1034, 5087, 1883, 3984, 1307,  871,   60,  538,  475, 5246,\n",
      "         506, 1081])\n",
      "==========Next Input==========\n",
      "tensor([6249, 6279,  580, 1786,  907, 5667,  341,  865, 2279,   88, 6614, 3224,\n",
      "        5307, 6540, 2408, 6202, 1771, 2218, 2140, 1639, 1603, 2214,    9, 3545,\n",
      "        5960, 3125, 5888, 5654, 1041, 5243, 4439,  960,  133, 3260, 6383, 1023,\n",
      "        5170, 6251, 4800,  531, 3010, 1176, 3069, 5891,  755,  202,  765, 4719,\n",
      "        2151,  739])\n",
      "==========Mask==========\n",
      "[ 3 11 19 36] [ 3  5 10]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([6249, 6279,  580, 1786,  907, 5667,  341,  865, 2279, 5920, 6614, 3224,\n",
      "        5307, 6540, 2408, 6202, 1771, 2218, 2140, 1288, 1603, 2214,    9, 3545,\n",
      "        5960, 3125, 5888, 5654, 1041,  481, 4439,  960,  133, 3260, 6383, 1023,\n",
      "        5170, 6251, 4800,  531, 5265, 1176, 3069, 5891,  755,  202,  765, 4719,\n",
      "        2151,  739])\n",
      "==========Test Input==========\n",
      "tensor([6249, 6279,  580, 4854,  907, 5238,  341,  865, 2279,   88, 5076,  981,\n",
      "        5307, 5384,  236, 2543, 1583, 2218, 2140,  925,  808, 2214,    9, 3545,\n",
      "        2216, 3125, 1990, 4265, 6413, 5243, 4439,  235, 3238, 3260, 6383,   90,\n",
      "        2952,  688, 4800,  531, 1907, 1176,  171, 1267, 5475,  780,  765, 2385,\n",
      "        1249,  739])\n",
      "==========Next Input==========\n",
      "tensor([2597, 1877, 3711, 5820, 2158, 5390,  248, 1520,  699, 5913, 5273, 2321,\n",
      "        5298, 6541, 3146,  772, 4193,  635, 3237, 6332,  764, 4005, 1870, 1736,\n",
      "        6792, 4572,  510, 1786,  202, 3085, 5407,  648,  880, 4728,  546,  218,\n",
      "           5, 2996, 4234, 1888,  523,  624, 3163, 5301, 3117,  218, 1444, 3433,\n",
      "        2426, 1639])\n",
      "==========Mask==========\n",
      "[ 9 19 29 40] [14 36]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([2597, 1877, 3711, 5820, 2158, 5390,  248, 1520,  699, 4593, 5273, 2321,\n",
      "        5298, 6541, 3146,  772, 4193,  635, 3237, 6332,  764, 4005, 1870, 1736,\n",
      "        6792, 2121,  510, 1786,  202, 3085, 3579,  648,  880, 3886,  613,  218,\n",
      "           5, 2996, 4234, 1888,  523,  624, 3163, 5301, 3117,  218, 1444, 3433,\n",
      "        2426, 1639])\n",
      "==========Test Input==========\n",
      "tensor([2597, 1877, 3711,   35, 2158, 2581,  248, 1520,  699, 2320, 6614, 1695,\n",
      "        5298, 5667, 2599, 5420, 6540,  635, 3237, 6202, 1603, 4005, 1870, 1736,\n",
      "        1771, 4572, 5888, 5654, 5960, 5920, 5407, 1041, 1288, 4728,  546, 1023,\n",
      "        1786, 6251, 4234, 1888,  133,  624, 3069,  960,  755,  202, 1444, 4719,\n",
      "        2151, 1639])\n",
      "==========Next Input==========\n",
      "tensor([ 781, 5575, 5475, 3978, 1453, 5674, 4524,  823, 2585, 5893, 4731, 2520,\n",
      "        5357, 3158, 4773,  924, 2684, 4144,  202, 5877,  350,   88, 4269,  411,\n",
      "         976, 3236, 5449, 6697, 1786, 3344,  692, 4234, 2588, 2923, 6158,  931,\n",
      "        3844, 3630, 2282, 4751, 5371, 3989, 6158, 5357, 6374,  180,  832, 3945,\n",
      "         381, 2645])\n",
      "==========Mask==========\n",
      "[ 9 25 30 33 34] [ 5  9 15 29 32]\n",
      "==============================\n",
      "==========Valid Input==========\n",
      "tensor([ 781, 5575, 5475, 6013, 1453, 5674, 4524,  823, 2585, 5893, 4731, 2520,\n",
      "        6743, 3158, 4773,  924, 2684, 4144,  202, 5877,  350,   88, 4269,  411,\n",
      "         976, 3236, 5095,  685, 1786, 3344,  692, 4234, 2588, 5381, 6158,  931,\n",
      "        3844, 3630, 2282, 4751, 1126, 3989, 6158, 5357, 6374,  180,  832, 3945,\n",
      "         381, 2645])\n",
      "==========Test Input==========\n",
      "tensor([ 781, 5575, 5475, 5891, 1453, 3224, 4524,  823, 2585, 1639, 5273, 3010,\n",
      "        5357, 5390, 2408, 5170, 6541, 4144,  202,  772,  764,   88, 4269,  411,\n",
      "        4193,  481,  510, 1786, 6792, 5913, 5265,  202, 6332, 4593, 2121,  218,\n",
      "        5820, 2996, 2282, 4751,  880, 3989, 3163,  648, 3117,  218,  832, 3433,\n",
      "        2426, 2645])\n",
      "==========Next Input==========\n",
      "tensor([5811, 4439, 5696, 6503,  235, 5468, 3644, 1685, 4607, 5809, 1691, 1177,\n",
      "         871, 5652, 5509, 6798, 2860, 2901, 1625, 6278,   85,  862, 2247, 2121,\n",
      "        4420,  398, 5764, 2674, 1974, 5036, 1336,  248,   31, 6736, 4725,  403,\n",
      "        4255, 3556,  774, 3810, 1292,  897, 1786, 5278, 1516,  402,  580, 5509,\n",
      "         373, 2529])\n",
      "==========Mask==========\n",
      "[ 3 12 26 27 33 40] [25 30 33 34]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, ((vinput, vtarget, vmask), (tinput, tmask))  in enumerate(zip(valid_loader, test_loader)):\n",
    "    if i<10:\n",
    "        print('='*30)\n",
    "        print('='*10+'Valid Input'+'='*10)\n",
    "        print(vinput)\n",
    "        print('='*10+'Test Input'+'='*10)\n",
    "        print(tinput)\n",
    "        print('='*10+'Next Input'+'='*10)\n",
    "        print(vtarget)\n",
    "        print('='*10+'Mask'+'='*10)\n",
    "        print(vmask, tmask)\n",
    "# for i, t  in enumerate(test_loader):\n",
    "#     print(1)\n",
    "#     if i==2:\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10275"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i, (tinput, tmask)  in enumerate(test_loader):\n",
    "    cnt+=1\n",
    "cnt\n",
    "# for i, t  in enumerate(test_loader):\n",
    "#     print(1)\n",
    "#     if i==2:\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9944"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GRU4REC(input_size=len(train_dataset.items)).to(device)\n",
    "criterion = TOP1_max()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 88995/92057 [05:24<00:11, 274.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01946418443238965 (0/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9944/10353 [00:10<00:00, 968.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.019254385100150134, Valid recall@10: 0.056534593724859215 (0/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() \n",
    "    train_losses = []\n",
    "    train_loader = SessionDataLoader(train_dataset)\n",
    "    \n",
    "    hidden = model.init_hidden()\n",
    "    for ii, (input, target, mask) in tqdm(enumerate(train_loader), total=len(train_loader.dataset.df) // train_loader.batch_size, miniters = 1000):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = reset_hidden(hidden, mask).detach()\n",
    "        logit, hidden = model(input, hidden)\n",
    "        \n",
    "        logit_sampled = logit[:, target.view(-1)]\n",
    "        loss = criterion(logit_sampled)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_mean_losses = np.mean(train_losses)\n",
    "    print(f'Train loss: {train_mean_losses} ({epoch}/{n_epochs})')\n",
    "\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    recalls = []\n",
    "    valid_loader = SessionDataLoader(valid_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden()\n",
    "        for ii, (input, target, mask) in tqdm(enumerate(valid_loader), total=len(valid_loader.dataset.df) // valid_loader.batch_size, miniters = 1000):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            logit, hidden = model(input, hidden)\n",
    "            logit_sampled = logit[:, target.view(-1)]\n",
    "            loss = criterion(logit_sampled)\n",
    "            recall = get_recall(logit, target)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "            recalls.append(recall)\n",
    "            \n",
    "    valid_mean_losses = np.mean(valid_losses)\n",
    "    valid_mean_recall = np.mean(recalls)\n",
    "    print(f'Valid loss: {valid_mean_losses}, Valid recall@10: {valid_mean_recall} ({epoch}/{n_epochs})')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
